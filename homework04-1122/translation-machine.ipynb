{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 火炬上的深度学习（下）第四节：彩云小译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课后作业：实现一个「英翻中」的翻译模型，并评估效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现后的模型希望能够达到这种效果：\n",
    "\n",
    "```\n",
    "> en2cn(\"Unbalanced development is one of china s successful experiences over the past years.\")\n",
    "不平衡 发展 是 我国 最近 20 年 的 成功 经验 之一 .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设计与目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 实现注意力模型；\n",
    "* 使用SRU的神经元；\n",
    "* 实现Beam Search算法来计算输出结果；\n",
    "* 实现「中翻英」，评估多次翻译后结果；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as DataSet\n",
    "\n",
    "# SRU包\n",
    "# 参照：https://github.com/taolei87/sru\n",
    "from sru import SRU, SRUCell\n",
    "\n",
    "# 绘图所用的包\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 尝试使用GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "itype = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "\n",
    "# 即时绘图\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里希望能够通过一定的封装来在不同的数据集上训练模型。因此希望能够做到在选用「对照数据集」和给定「句子长度限制」后得到可供模型使用的训练集、校验集、测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import jieba\n",
    "import random\n",
    "\n",
    "class Lang:\n",
    "    SOS = 0\n",
    "    EOS = 1\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.wordNumb = 2\n",
    "        # 这里没有使用list结构，因为SOS与EOS为非可见的控制字符，防止与文本中的SOS或者EOS混淆\n",
    "        self.idx2word = {Lang.SOS:'SOS', Lang.EOS:'EOS'}\n",
    "        self.word2idx = {}\n",
    "        self.wordFreq = {}\n",
    "        \n",
    "    def addWord(self, word):\n",
    "        if word in self.word2idx:\n",
    "            self.wordFreq[word] += 1\n",
    "        else:\n",
    "            self.idx2word[self.wordNumb]=word\n",
    "            self.word2idx[word]=self.wordNumb\n",
    "            self.wordFreq[word]=1\n",
    "            self.wordNumb += 1\n",
    "            \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def word2index(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.addWord(word)\n",
    "        return self.word2idx[word];\n",
    "    \n",
    "    def sen2indexes(self, sentence):\n",
    "        return [self.word2index(w) for w in sentence.split(' ')]\n",
    "    \n",
    "    def index2word(self, index):\n",
    "        return self.idx2word[index] if index > 0 and index < self.wordNumb else ''\n",
    "    \n",
    "    def indexes2sen(self, indexes):\n",
    "        return ' '.join([self.index2word(i) for i in indexes if i != Lang.EOS])\n",
    "    \n",
    "    def wordNum(self):\n",
    "        return self.wordNumb\n",
    "        \n",
    "\n",
    "\n",
    "class LangData:\n",
    "    \n",
    "    def __init__(self, chinese_file, english_file, encoding='utf-8', cut_chinese=False):\n",
    "        chinese = open(chinese_file,'r',encoding=encoding).read().strip().split('\\n')\n",
    "        if cut_chinese:\n",
    "            self.chinese = [self.cutChinese(s) for s in chinese]\n",
    "        else:\n",
    "            self.chinese = chinese\n",
    "        \n",
    "        english = open(english_file,'r',encoding=encoding).read().strip().split('\\n')\n",
    "        self.english = [self.normalizeEnglish(s) for s in english]\n",
    "\n",
    "           \n",
    "    # Turn a Unicode string to plain ASCII, thanks to\n",
    "    # http://stackoverflow.com/a/518232/2809427\n",
    "    # 将unicode编码转变为ascii编码\n",
    "    def u2a(self, s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "    \n",
    "    def normalizeEnglish(self, s):\n",
    "        s = self.u2a(s)\n",
    "        s = re.sub(r\"(\\W)\", r\" \\1\",s)\n",
    "        s = re.sub(r\"[^a-zA-Z0-9_,\\.\\!\\?]+\", r\" \", s)\n",
    "        s = s.lower().strip()\n",
    "        return s\n",
    "    \n",
    "    def cutChinese(self, s):\n",
    "        return \" \".join(jieba.cut(s))\n",
    "    \n",
    "    def _toIndex(self, l, s, n):\n",
    "        idx = l.sen2indexes(s)\n",
    "        for i in range(n - len(idx)):\n",
    "            idx.append(Lang.EOS)\n",
    "        return idx\n",
    "    \n",
    "    def getDataLoader(self, sen_length, batch_size, target='cn'):\n",
    "        # 构造语言对，并打乱\n",
    "        pairs = [p \n",
    "                 for p in zip(self.chinese, self.english) \n",
    "                 if (len(p[0].split(' ')) < sen_length and len(p[1].split(' ')) < sen_length)]\n",
    "        \n",
    "        random.shuffle(pairs)\n",
    "                \n",
    "        # 建立词索引\n",
    "        Chinese, English = Lang('Chinese'), Lang('English')\n",
    "        for c,e in pairs:\n",
    "            Chinese.addSentence(c)\n",
    "            English.addSentence(e)\n",
    "\n",
    "        # 文本变为索引，并且扩展为等长长度\n",
    "        pairs = [(self._toIndex(Chinese,p[0],sen_length), self._toIndex(English,p[1],sen_length)) for p in pairs]\n",
    "\n",
    "        xi,yi = (1,0) if target=='cn' else (0,1)\n",
    "        \n",
    "        # 形成训练集、校验集和测试集\n",
    "        valid_size = len(pairs) // 10\n",
    "        if valid_size > 10000:\n",
    "            valid_size = 10000\n",
    "            \n",
    "        train = torch.LongTensor(pairs[ : -valid_size])\n",
    "        valid = torch.LongTensor(pairs[-valid_size : -valid_size // 2])\n",
    "        test  = torch.LongTensor(pairs[-valid_size // 2 :])\n",
    "        \n",
    "        # 形成训练集\n",
    "        trainSet = DataSet.TensorDataset(train[:,xi], train[:,yi])\n",
    "        trainLoader = DataSet.DataLoader(trainSet, batch_size = batch_size, shuffle=True, num_workers=8)\n",
    "        # 校验数据\n",
    "        validSet = DataSet.TensorDataset(valid[:,xi], valid[:,yi])\n",
    "        validLoader = DataSet.DataLoader(validSet, batch_size = batch_size, shuffle=True, num_workers=8)\n",
    "        # 测试数据\n",
    "        testSet = DataSet.TensorDataset(test[:,xi], test[:,yi])\n",
    "        testLoader = DataSet.DataLoader(testSet, batch_size = batch_size, shuffle=True, num_workers=8)\n",
    "        \n",
    "        print('有效句子对：{}\\n训练记录：{}\\n校验记录：{}\\n测试记录：{}'\n",
    "              .format(len(pairs),len(trainSet),len(validSet),len(testSet)))\n",
    "        \n",
    "        return trainLoader, validLoader, testLoader, Chinese, English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里得到得到训练集、验证集、测试集以及语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文句子总数：100000\n",
      "英文句子总数：100000\n"
     ]
    }
   ],
   "source": [
    "langData=LangData('data/chinese.txt','data/english.txt')\n",
    "\n",
    "print('中文句子总数：{}\\n英文句子总数：{}'.format(len(langData.chinese),len(langData.english)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效句子对：49993\n",
      "训练记录：44994\n",
      "校验记录：2499\n",
      "测试记录：2500\n",
      "总单词数：\n",
      "  CH: 33483\n",
      "  EN: 22436\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 32\n",
    "train, valid, test, Chi, Eng = langData.getDataLoader(MAX_LENGTH, batch_size = 256, target='cn')\n",
    "\n",
    "print(\"总单词数：\\n  CH: {}\\n  EN: {}\".format(Chi.wordNum(),Eng.wordNum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建基于AM模型的神经网络，编码器与解码器都使用SRU作为激活单元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, core='sru'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.is_sru = core == 'sru'\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        if self.is_sru:\n",
    "            self.rnn = SRU(hidden_size, hidden_size,\n",
    "                num_layers = n_layers,          # number of stacking RNN layers\n",
    "                dropout = 0.0,           # dropout applied between RNN layers\n",
    "                rnn_dropout = 0.0,       # variational dropout applied on linear transformation\n",
    "                use_tanh = 0,         # use tanh?\n",
    "                use_relu = 0,            # use ReLU?\n",
    "                bidirectional = True    # bidirectional RNN ?\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, num_layers = n_layers, bidirectional = True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input尺寸： batch_size, length_seq\n",
    "        # embedded尺寸：batch_size, length_seq, hidden_size\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # embedded转成：seq_size, batch_size, hidden_size\n",
    "        input = embedded.transpose(0,1).contiguous()\n",
    "        \n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "\n",
    "        # output转为：batch_size, length_seq, hidden_size\n",
    "        output = output.transpose(0,1).contiguous()\n",
    "        \n",
    "        # hidden：num_layers, batch_size, hidden_size * directions\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        if self.is_sru:\n",
    "            return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size * 2).type(dtype))\n",
    "        else:\n",
    "            return Variable(torch.zeros(self.n_layers * 2, batch_size, self.hidden_size).type(dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, attn_size, n_layers=1, dropout_p=0.1, core='sru'):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.is_sru = core == 'sru'\n",
    "\n",
    "        # 词嵌入层\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        \n",
    "        # 注意力网络（一个前馈神经网络）\n",
    "        self.attn = nn.Linear(hidden_size * (2 * n_layers + 1), attn_size)\n",
    "    \n",
    "        # 注意力机制作用完后的结果映射到后面的层\n",
    "        self.attn_combine = nn.Linear(hidden_size * 3, hidden_size)\n",
    "        \n",
    "        # dropout操作层\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        if self.is_sru:\n",
    "            self.rnn = SRU(hidden_size, hidden_size,\n",
    "                num_layers = n_layers,          # number of stacking RNN layers\n",
    "                dropout = dropout_p,           # dropout applied between RNN layers\n",
    "                rnn_dropout = dropout_p,       # variational dropout applied on linear transformation\n",
    "                use_tanh = 0,         # use tanh?\n",
    "                use_relu = 0,            # use ReLU?\n",
    "                bidirectional = True    # bidirectional RNN ?\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, num_layers = n_layers, bidirectional = True)\n",
    "            \n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input大小：batch_size, length_seq=1\n",
    "        # hidden大小：n_layer, batch_size, hidden_size*direction\n",
    "        \n",
    "        # embedded大小：batch_size, length_seq, hidden_size\n",
    "        embedded = self.embedding(input)\n",
    "        # embedded大小：batch_size, hidden_size\n",
    "        embedded = embedded[:, 0, :]\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # 将hidden张量数据转化成batch_size排在第0维的形状\n",
    "        attn_input_hidden = hidden.transpose(0, 1).contiguous()\n",
    "        # hidden_attn大小：batch_size, direction*n_layers*hidden_size\n",
    "        attn_input_hidden = attn_input_hidden.view(attn_input_hidden.size(0), -1)\n",
    "        \n",
    "        # attn_input：batch_size, hidden_size * (1 + direction * n_layers)\n",
    "        attn_input = torch.cat((embedded, attn_input_hidden), 1)\n",
    "        \n",
    "        # 注意力层输出的权重\n",
    "        # attn_weights大小：batch_size, max_length\n",
    "        attn_weights = F.softmax(self.attn(attn_input))\n",
    "        \n",
    "        # 当输入数据不标准的时候，对weights截取必要的一段\n",
    "        # attn_weights大小：batch_size, length_seq_of_encoder\n",
    "        attn_weights = attn_weights[:, : encoder_outputs.size(1)]\n",
    "        # attn_weights大小：batch_size, 1, length_seq 中间的1是为了bmm乘法用的\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        \n",
    "        # 将attention的weights矩阵乘encoder_outputs以计算注意力完的结果\n",
    "        # encoder_outputs大小：batch_size, seq_length, hidden_size * 2\n",
    "        # attn_applied大小：batch_size, 1, hidden_size * 2\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_outputs) \n",
    "        \n",
    "        # 将输入的词向量与注意力机制作用后的结果拼接成一个大的输入向量\n",
    "        # output大小：batch_size, hidden_size * (direction + 1)\n",
    "        to_combine = torch.cat((embedded, attn_applied[:,0,:]), 1)\n",
    "        \n",
    "        # 将大输入向量映射为GRU的输入向量\n",
    "        # input：batch_size, 1, hidden_size\n",
    "        input = self.attn_combine(to_combine).unsqueeze(1)\n",
    "        \n",
    "        input = F.relu(input)\n",
    "        input = self.dropout(input)\n",
    "\n",
    "        # 开始解码器SRU的运算\n",
    "        # output大小：length_seq, batch_size, hidden_size * directions\n",
    "        # hidden大小：n_layers, batch_size, hidden_size * directions\n",
    "        input = input.transpose(0, 1).contiguous()\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        \n",
    "        # output大小：batch_size * output_size\n",
    "        output = self.fc(output[-1, :, :])\n",
    "        \n",
    "        output = F.log_softmax(output)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    \n",
    "    def initInput(self, batch_size):\n",
    "        return Variable(torch.LongTensor([[Lang.SOS]] * batch_size).type(itype))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def time_since(t):\n",
    "    now = time.time()\n",
    "    s = now - t\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def rightness(output, labels):\n",
    "    # predictions: batch, max_length, class_num\n",
    "#     y = torch.max(predictions.data, 1)[1]\n",
    "    rights = output.eq(labels.data).sum()\n",
    "    return rights, labels.numel()\n",
    "\n",
    "def beam_search(bn, decoder, input, hidden, encoder_outputs, max_length=MAX_LENGTH, debug=True):\n",
    "    batch_num = input.size(0)\n",
    "\n",
    "    # 执行第一步beam_search\n",
    "    decoder_output, hidden, attn = decoder(input, hidden, encoder_outputs)\n",
    "    topv, topi = decoder_output.data.topk(bn, 1)\n",
    "    \n",
    "    # 权重w存储每个baam的权重和：batch, bn\n",
    "    w = topv\n",
    "    # output: batch, bn, max_length\n",
    "    outputs = (topi.cpu() if use_cuda else topi).unsqueeze(2)\n",
    "     \n",
    "    # hiddens: bn, ...\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "    hiddens = hidden.clone()\n",
    "    for i in range(1, bn):\n",
    "        hiddens = torch.cat((hiddens, hidden.clone()), 0)\n",
    "    \n",
    "    # attns: batch, bn, max_length, max_length\n",
    "    attn = attn.unsqueeze(1).data\n",
    "    attns = attn.clone()\n",
    "    for i in range(1, bn):\n",
    "        attns = torch.cat((attns, attn.clone()), 1)\n",
    "        \n",
    "    if debug:\n",
    "        print('---0---')\n",
    "        print(\"\\n\".join([\"({:.4f}) {:s}\".format(w[0,i], Chi.indexes2sen(outputs[0,i,:].numpy())) for i in range(bn)]))\n",
    "    \n",
    "    # 循环执行后续步骤\n",
    "    for si in range(1, max_length):\n",
    "        # b_w: batch, bn*class_num\n",
    "        b_w=torch.FloatTensor().type(dtype)\n",
    "        b_a=torch.FloatTensor().type(dtype)\n",
    "        \n",
    "        for bi in range(bn):\n",
    "            input = Variable(outputs[:,bi,-1].unsqueeze(1)).type(itype)\n",
    "            decoder_output, hiddens[bi], attn = decoder(input, hiddens[bi,...], encoder_outputs)\n",
    "#             print(w[:,bi].size(), decoder_output.data.size())\n",
    "            b_w = torch.cat((b_w, (w[:,bi].unsqueeze(1) + decoder_output.data)), 1)\n",
    "            b_a = torch.cat((b_a, attn.unsqueeze(1).data), 1)\n",
    "        \n",
    "        n = decoder_output.size(1)\n",
    "        topv, topi = b_w.topk(bn, 1)\n",
    "        w = topv\n",
    "        \n",
    "        # 拼接结果\n",
    "        _o = outputs.numpy().tolist()\n",
    "        outputs = []\n",
    "        for i in range(batch_num):\n",
    "            _bn = []\n",
    "            for j in range(bn):\n",
    "                t = _o[i][topi[i, j] // n][:] if si!=0 else []\n",
    "                t.append(topi[i, j] % n)\n",
    "                _bn.append(t)\n",
    "            outputs.append(_bn)\n",
    "        outputs = torch.LongTensor(outputs)\n",
    "        attns = torch.cat((attns, b_a), 2)\n",
    "        \n",
    "        if debug:\n",
    "            print('---{}---'.format(si))\n",
    "            print(\"\\n\".join([\"({:.4f}) {:s}\".format(w[0,i], Chi.indexes2sen(outputs[0,i,:].numpy())) for i in range(bn)]))\n",
    "\n",
    "    maxwi = w.max(1)[1]\n",
    "    outputs = outputs.numpy()\n",
    "    attns = (attns.cpu() if use_cuda else attns).numpy()\n",
    "    output = [outputs[i][maxwi[i]] for i in range(batch_num)]\n",
    "    attn = [attns[i][maxwi[i]] for i in range(batch_num)]\n",
    "    \n",
    "    return output, attn, outputs, attns, w\n",
    "\n",
    "def evaluate(sentence, encoder, decoder, bn=3,  max_length=MAX_LENGTH , beam=True, debug=False):\n",
    "    if sentence.dim() == 1:\n",
    "        sentence = sentence.unsqueeze(0)\n",
    "    batch_size = sentence.size(0)\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "        \n",
    "    input = Variable(sentence)\n",
    "#     input = sentence\n",
    "    hidden = encoder.initHidden(batch_size)\n",
    "    \n",
    "    encoder_outputs, hidden = encoder(input, hidden)\n",
    "    \n",
    "    input = decoder.initInput(batch_size)\n",
    "    \n",
    "    # 使用beam_search算法\n",
    "    if beam:\n",
    "        return beam_search(bn, decoder, input, hidden, encoder_outputs, debug=debug)\n",
    "\n",
    "    # 使用最大概率生成\n",
    "    output = torch.LongTensor().type(itype)\n",
    "    attns = torch.FloatTensor().type(dtype)\n",
    "    for i in range(max_length):\n",
    "        decoder_output, hidden, attn = decoder(input, hidden, encoder_outputs)\n",
    "        output= torch.cat((output, decoder_output.data.topk(1,1)[1]), 1)\n",
    "        attns = torch.cat((attns, attn.data), 0)\n",
    "        input = Variable(output[:,-1]).unsqueeze(1)\n",
    "        \n",
    "    output = (output.cpu() if use_cuda else output).numpy()\n",
    "    attns = (attns.cpu() if use_cuda else attns).numpy()\n",
    "    return output, attns, [],[],[]\n",
    "    \n",
    "    \n",
    "        \n",
    "def training(encoder, decoder, train_loader, valid_loader, log=[], n_epoch=100, lr=0.001, max_length=MAX_LENGTH):\n",
    "    \n",
    "    teacher_forcing_ratio = 0.5\n",
    "    \n",
    "    # 为两个网络分别定义优化器\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "    # 定义损失函数\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = 0\n",
    "        # 对训练数据循环\n",
    "        for data in train_loader:\n",
    "            input_variable = Variable(data[0]).type(itype)\n",
    "            # input_variable的大小：batch_size, length_seq\n",
    "            target_variable = Variable(data[1]).type(itype)\n",
    "            # target_variable的大小：batch_size, length_seq\n",
    "\n",
    "            # 初始化编码器状态\n",
    "            encoder_hidden = encoder.initHidden(data[0].size(0))\n",
    "            # 清空梯度\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "            # 开始解码器的工作\n",
    "            decoder_input = decoder.initInput(target_variable.size(0))\n",
    "\n",
    "            # 传递隐藏层状态\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            # 以teacher_forcing_ratio的比例用target中的翻译结果作为监督信息\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                for di in range(MAX_LENGTH):            \n",
    "                    decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                    loss += criterion(decoder_output, target_variable[:, di])\n",
    "                    decoder_input = target_variable[:, di].unsqueeze(1)  # Teacher forcing\n",
    "\n",
    "            else:\n",
    "                for di in range(MAX_LENGTH):\n",
    "                    decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                    loss += criterion(decoder_output, target_variable[:, di])\n",
    "                    topv, topi = decoder_output.data.topk(1, 1)\n",
    "                    decoder_input = Variable(topi).type(itype)\n",
    "\n",
    "            # 开始反向传播\n",
    "            loss.backward()\n",
    "\n",
    "            # 开始梯度下降\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            # 累加总误差\n",
    "            train_loss += (loss.cpu() if use_cuda else loss).data.numpy()[0]\n",
    "\n",
    "        # 计算训练时候的平均误差\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "\n",
    "        \n",
    "        #### 开始跑校验数据集\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        valid_loss = 0\n",
    "        rights = []\n",
    "        # 对校验数据集循环\n",
    "        for data in valid_loader:\n",
    "            loss = 0\n",
    "            input_variable = Variable(data[0]).type(itype)\n",
    "            target_variable = Variable(data[1]).type(itype)\n",
    "            encoder_hidden = encoder.initHidden(data[0].size(0))\n",
    "            \n",
    "            encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "            decoder_input = decoder.initInput(target_variable.size(0))\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            for di in range(MAX_LENGTH):\n",
    "                \n",
    "                decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "                topv, topi = decoder_output.data.topk(1, 1)\n",
    "                decoder_input = Variable(topi).type(itype)\n",
    "\n",
    "                right = rightness(topi[:,0], target_variable[:, di])\n",
    "                rights.append(right)\n",
    "\n",
    "                # 计算损失函数\n",
    "                loss += criterion(decoder_output, target_variable[:, di])\n",
    "            valid_loss += (loss.cpu() if use_cuda else loss).data.numpy()[0]\n",
    "            \n",
    "        valid_loss = valid_loss / len(valid_loader)\n",
    "            \n",
    "        \n",
    "        # 打印每一个Epoch的输出结果\n",
    "        rights = np.array(rights)\n",
    "        right_ratio = rights[:,0].sum() / rights[:,1].sum()\n",
    "\n",
    "        print('进程：{:.1f}% 训练损失：{:.4f}，校验损失：{:.4f}，词正确率：{:.2f}% ({})'\n",
    "              .format((epoch+1) / n_epoch * 100, train_loss, valid_loss, right_ratio * 100.0, time_since(start)))\n",
    "        log.append([train_loss, valid_loss , right_ratio])\n",
    "        \n",
    "    return log\n",
    "\n",
    "def draw_log(log):\n",
    "    a = [i[0] for i in log]\n",
    "    b = [i[1] for i in log]\n",
    "    c = [i[2] * 100 for i in log]\n",
    "    plt.plot(a, label = 'Training Loss')\n",
    "    plt.plot(b, label = 'Validation Loss')\n",
    "    plt.plot(c, label = 'Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss & Accuracy')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进程：1.0% 训练损失：213.7781，校验损失：142.9037，词正确率：47.02% (0m 52s)\n",
      "进程：2.0% 训练损失：140.5021，校验损失：137.6264，词正确率：45.36% (1m 43s)\n",
      "进程：3.0% 训练损失：134.1194，校验损失：164.9646，词正确率：14.65% (2m 36s)\n",
      "进程：4.0% 训练损失：130.8302，校验损失：163.0115，词正确率：19.10% (3m 29s)\n",
      "进程：5.0% 训练损失：128.0687，校验损失：148.6449，词正确率：33.61% (4m 21s)\n",
      "进程：6.0% 训练损失：127.4894，校验损失：144.1607，词正确率：37.00% (5m 13s)\n",
      "进程：7.0% 训练损失：126.4672，校验损失：141.6516，词正确率：39.77% (6m 6s)\n",
      "进程：8.0% 训练损失：125.8165，校验损失：140.6990，词正确率：39.83% (6m 59s)\n",
      "进程：9.0% 训练损失：125.5274，校验损失：141.7225，词正确率：39.07% (7m 52s)\n",
      "进程：10.0% 训练损失：124.5071，校验损失：136.5392，词正确率：43.58% (8m 44s)\n",
      "进程：11.0% 训练损失：124.1735，校验损失：136.6324，词正确率：42.83% (9m 36s)\n",
      "进程：12.0% 训练损失：123.5910，校验损失：136.1616，词正确率：43.83% (10m 29s)\n",
      "进程：13.0% 训练损失：123.0106，校验损失：135.3738，词正确率：44.77% (11m 21s)\n",
      "进程：43.0% 训练损失：112.3839，校验损失：129.1358，词正确率：48.71% (37m 38s)\n",
      "进程：44.0% 训练损失：111.6585，校验损失：129.0009，词正确率：48.65% (38m 30s)\n",
      "进程：45.0% 训练损失：112.5932，校验损失：126.8013，词正确率：49.48% (39m 23s)\n",
      "进程：46.0% 训练损失：111.5602，校验损失：126.7939，词正确率：49.45% (40m 16s)\n",
      "进程：47.0% 训练损失：111.0686，校验损失：128.6095，词正确率：48.77% (41m 9s)\n",
      "进程：48.0% 训练损失：110.1806，校验损失：127.9947，词正确率：48.99% (42m 1s)\n",
      "进程：49.0% 训练损失：110.6193，校验损失：128.0080，词正确率：48.97% (42m 54s)\n",
      "进程：50.0% 训练损失：110.1410，校验损失：126.6959，词正确率：49.49% (43m 46s)\n",
      "进程：51.0% 训练损失：110.1360，校验损失：127.4130，词正确率：49.09% (44m 39s)\n",
      "进程：52.0% 训练损失：109.0207，校验损失：126.9326，词正确率：49.25% (45m 32s)\n",
      "进程：53.0% 训练损失：109.2710，校验损失：127.2391，词正确率：49.20% (46m 25s)\n",
      "进程：54.0% 训练损失：108.1304，校验损失：126.9494，词正确率：49.15% (47m 17s)\n",
      "进程：55.0% 训练损失：108.3454，校验损失：127.3528，词正确率：48.96% (48m 10s)\n",
      "进程：56.0% 训练损失：106.9922，校验损失：126.6655，词正确率：49.21% (49m 2s)\n",
      "进程：57.0% 训练损失：107.4809，校验损失：126.4948，词正确率：49.30% (49m 54s)\n",
      "进程：58.0% 训练损失：107.2716，校验损失：126.0482，词正确率：49.43% (50m 47s)\n",
      "进程：59.0% 训练损失：105.5323，校验损失：127.5270，词正确率：48.74% (51m 39s)\n",
      "进程：60.0% 训练损失：106.3804，校验损失：126.0545，词正确率：49.39% (52m 31s)\n",
      "进程：61.0% 训练损失：105.4660，校验损失：127.4042，词正确率：48.64% (53m 24s)\n",
      "进程：62.0% 训练损失：105.4250，校验损失：126.5519，词正确率：49.07% (54m 16s)\n",
      "进程：63.0% 训练损失：104.6935，校验损失：127.2536，词正确率：48.79% (55m 8s)\n",
      "进程：64.0% 训练损失：104.4118，校验损失：126.0582，词正确率：49.25% (56m 1s)\n",
      "进程：65.0% 训练损失：104.1386，校验损失：127.7573，词正确率：48.42% (56m 53s)\n",
      "进程：66.0% 训练损失：104.5119，校验损失：127.9767，词正确率：48.38% (57m 46s)\n",
      "进程：67.0% 训练损失：103.3271，校验损失：126.7531，词正确率：48.87% (58m 38s)\n",
      "进程：68.0% 训练损失：103.0521，校验损失：125.5764，词正确率：49.46% (59m 31s)\n",
      "进程：69.0% 训练损失：102.0230，校验损失：126.9182，词正确率：49.02% (60m 22s)\n",
      "进程：70.0% 训练损失：101.8478，校验损失：126.9066，词正确率：48.83% (61m 14s)\n",
      "进程：71.0% 训练损失：102.5960，校验损失：128.4794，词正确率：48.07% (62m 7s)\n",
      "进程：72.0% 训练损失：100.2597，校验损失：127.2305，词正确率：48.73% (62m 58s)\n",
      "进程：73.0% 训练损失：101.9762，校验损失：126.0434，词正确率：49.24% (63m 51s)\n",
      "进程：74.0% 训练损失：100.7345，校验损失：126.2399，词正确率：49.17% (64m 44s)\n",
      "进程：75.0% 训练损失：99.9782，校验损失：127.3918，词正确率：48.55% (65m 35s)\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 80\n",
    "n_layers = 1\n",
    "core = 'sru'\n",
    "\n",
    "encoder = Encoder(Eng.wordNum(), hidden_size, n_layers = n_layers, core=core)\n",
    "decoder = Decoder(hidden_size, Chi.wordNum(), n_layers = n_layers , attn_size=MAX_LENGTH, dropout_p=0.3, core=core)\n",
    "if use_cuda:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "log = []\n",
    "log = training(encoder, decoder, train, valid, log=log, max_length=MAX_LENGTH, lr=0.0001, n_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMDOzWu4gzMysljsIMzOr5Q7CzMxq/QN9mwJI6J5xUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f41b4673748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_log(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCcI551xDniCcc8419H8KAExE9rT7tAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa4c3459cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_log(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取测试集的第一个batch，输出机器翻译结果和标准答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for td in test:\n",
    "    x = td[0].type(itype)\n",
    "    y = td[1].type(itype)\n",
    "    \n",
    "    break\n",
    "output, attn, outputs, attns, w = evaluate(x, encoder, decoder, bn=1,  max_length=MAX_LENGTH, beam=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入<：solve problem engendered yat asked examination .\n",
      "输出>：有待 江泽民 江泽民 一致 一致\n",
      "标准=：执迷不悟 惊恐 人口 尽管 江泽民 整顿 一致\n",
      "\n",
      "输入<：gloomy reporter s educating factory difficulties it comrade .\n",
      "输出>：针锋相对 针锋相对 之大 何种 恰恰 恰恰 恰恰 一致\n",
      "标准=：斗争 农村 宴会厅 一致\n",
      "\n",
      "输入<：recalled concurrently the a sides municipality .\n",
      "输出>：必胜 当 当 县 一致 一致\n",
      "标准=：屁股 拍手称快 县 自身 老 当 作用 中方 一致\n",
      "\n",
      "输入<：, open its shahi .\n",
      "输出>：而且 当 县 谴责 一致\n",
      "标准=：以往 网络 所在 一致\n",
      "\n",
      "输入<：, stated shown it companies uncle .\n",
      "输出>：情形 情形 路 亚洲 随之 随之 一致\n",
      "标准=：在外 大敌 威胁 厅 整顿 注重 悠久 一致\n",
      "\n",
      "输入<：state clearly equipment thinking base identical\n",
      "输出>：受托 农业 农业 农业 农业 状态\n",
      "标准=：农业 美国 低估 发明 阻碍 状态\n",
      "\n",
      "输入<：warnings .\n",
      "输出>：图书馆 坚定性 解决好 解决好 解决好 解决好 解决好 解决好\n",
      "标准=：迷失 但 方向 看到\n",
      "\n",
      "输入<：more thoughts solved 14 accompanied fry harmful .\n",
      "输出>：宴会厅 厅 厅 县 县 县 一致 一致\n",
      "标准=：阮国琴 军工 厅 二零零八年 一致\n",
      "\n",
      "输入<：including jointly equipment its not stage 80 .\n",
      "输出>：厂 当 当 当 县 县 一致\n",
      "标准=：不 . 建 建筑 一致\n",
      "\n",
      "输入<：smile population economy altering invitation textbooks strait\n",
      "输出>：难怪 难怪 难怪 难怪 单位 单位 洛杉矶 洛杉矶 热 热\n",
      "标准=：相似 下台 遭遇 根本\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 9\n",
    "# print(output[i])\n",
    "# print(\"\\n\".join([\"{:d}.[{:.4f}] > {:s}\".format(j, w[i][j], Chi.indexes2sen(outputs[i][j])) for j in range(3)]))\n",
    "for i in random.sample(range(x.size(0)), 10):\n",
    "    print(\"输入<：{}\\n输出>：{}\\n标准=：{}\\n\".format(\n",
    "        Eng.indexes2sen(x[i].cpu().numpy()),\n",
    "        Chi.indexes2sen(output[i]), \n",
    "        Chi.indexes2sen(y[i,:].cpu().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 封装工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def en2cn(sentence, bn=1, debug=False):\n",
    "    x = torch.LongTensor(langData._toIndex(Eng,sentence,MAX_LENGTH)).type(itype)\n",
    "    output, attn, outputs, attns, w = evaluate(x, encoder, decoder, bn=bn,  max_length=MAX_LENGTH, debug=debug) \n",
    "    print(\"\\n\".join([\"> [{:.4f}] {:s}\".format(w[0][j], Chi.indexes2sen(outputs[0][j])) for j in range(bn)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用测试，当 bn 设置为 1 的时候，beam_search 退化为经典最大值算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [-5.3089] 代表 认识 , 了解 监督 .\n"
     ]
    }
   ],
   "source": [
    "en2cn('enhance understanding and unify thinking .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试输出概率最大的三个翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> [-5.1817] 代表 认识 了解 , 意识 .\n",
      "> [-5.2232] 代表 深刻 , 了解 监督 .\n",
      "> [-5.6519] 代表 认识 , 了解 , 意识 .\n"
     ]
    }
   ],
   "source": [
    "en2cn('enhance understanding and unify thinking .', bn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 Beam_search 一步步是如何工作的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---0---\n",
      "(-0.6119) 代表\n",
      "(-1.5805) 三\n",
      "(-1.6381) 加强\n",
      "---1---\n",
      "(-1.6041) 代表 认识\n",
      "(-1.8916) 代表 深刻\n",
      "(-2.0122) 代表 加强\n",
      "---2---\n",
      "(-2.5654) 代表 深刻 ,\n",
      "(-2.7179) 代表 认识 ,\n",
      "(-3.0411) 代表 认识 了解\n",
      "---3---\n",
      "(-3.6788) 代表 认识 了解 ,\n",
      "(-3.6985) 代表 深刻 , 了解\n",
      "(-3.7443) 代表 认识 , 了解\n",
      "---4---\n",
      "(-4.0302) 代表 认识 , 了解 ,\n",
      "(-4.5215) 代表 深刻 , 了解 监督\n",
      "(-4.8960) 代表 认识 了解 , 意识\n",
      "---5---\n",
      "(-5.0025) 代表 深刻 , 了解 监督 .\n",
      "(-5.1008) 代表 认识 了解 , 意识 .\n",
      "(-5.4924) 代表 认识 , 了解 , 意识\n",
      "---6---\n",
      "(-5.1602) 代表 认识 了解 , 意识 .\n",
      "(-5.2148) 代表 深刻 , 了解 监督 .\n",
      "(-5.6374) 代表 认识 , 了解 , 意识 .\n",
      "---7---\n",
      "(-5.1793) 代表 认识 了解 , 意识 .\n",
      "(-5.2222) 代表 深刻 , 了解 监督 .\n",
      "(-5.6506) 代表 认识 , 了解 , 意识 .\n",
      "---8---\n",
      "(-5.1813) 代表 认识 了解 , 意识 .\n",
      "(-5.2231) 代表 深刻 , 了解 监督 .\n",
      "(-5.6517) 代表 认识 , 了解 , 意识 .\n",
      "---9---\n",
      "(-5.1816) 代表 认识 了解 , 意识 .\n",
      "(-5.2232) 代表 深刻 , 了解 监督 .\n",
      "(-5.6519) 代表 认识 , 了解 , 意识 .\n",
      "---10---\n",
      "(-5.1817) 代表 认识 了解 , 意识 .\n",
      "(-5.2232) 代表 深刻 , 了解 监督 .\n",
      "(-5.6519) 代表 认识 , 了解 , 意识 .\n",
      "---11---\n",
      "(-5.1817) 代表 认识 了解 , 意识 .\n",
      "(-5.2232) 代表 深刻 , 了解 监督 .\n",
      "(-5.6519) 代表 认识 , 了解 , 意识 .\n",
      "---12---\n",
      "(-5.1817) 代表 认识 了解 , 意识 .\n",
      "(-5.2232) 代表 深刻 , 了解 监督 .\n",
      "(-5.6519) 代表 认识 , 了解 , 意识 .\n",
      "---13---\n",
      "(-5.1817) 代表 认识 了解 , 意识 .\n",
      "(-5.2232) 代表 深刻 , 了解 监督 .\n",
      "(-5.6519) 代表 认识 , 了解 , 意识 .\n",
      "---14---\n",
      "(-5.1817) 代表 认识 了解 , 意识 .\n",
      "(-5.2232) 代表 深刻 , 了解 监督 .\n",
      "(-5.6519) 代表 认识 , 了解 , 意识 .\n",
      "---15---\n",
      "(-5.1817) 代表 认识 了解 , 意识 .\n",
      "(-5.2232) 代表 深刻 , 了解 监督 .\n",
      "(-5.6519) 代表 认识 , 了解 , 意识 .\n",
      "> [-5.1817] 代表 认识 了解 , 意识 .\n",
      "> [-5.2232] 代表 深刻 , 了解 监督 .\n",
      "> [-5.6519] 代表 认识 , 了解 , 意识 .\n"
     ]
    }
   ],
   "source": [
    "en2cn('enhance understanding and unify thinking .', bn=3, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存网络结构和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vtnil/anaconda3/envs/torch/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/vtnil/anaconda3/envs/torch/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(encoder, 'encoder-toy.pkl')\n",
    "torch.save(decoder, 'decoder-toy.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结本作业有几个收获：\n",
    "\n",
    "* 通过重新组织代码，梳理清晰了各部分的逻辑，同时增加了对 pytorch 的熟悉程度；\n",
    "* 通过使用 SRU，了解了 SRU 与 GRU 的对比情况；\n",
    "* 通过实现 Beam search 算法，增加了对各种数据类型操作的熟悉程度；\n",
    "* 中间还遇到一个小插曲，通过对问题的排查，增加了调试训练的能力；\n",
    "* 通过多参数跑数据集，了解到在这个实验中，注意力网络在中英翻译上的局限；\n",
    "* 基本实现了作业最初定的目标，「中翻英」以及进一步的实验因为翻译效果不理想，所以索性不再继续；\n",
    "* 16 词长度，使用本代码大概占用 1G 左右显存；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Debug\n",
    "\n",
    "尝试通过替换模块来调试程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        # 第一层Embeddeing\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # 第二层GRU，注意GRU中可以定义很多层，主要靠num_layers控制\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first = True, \n",
    "                          num_layers = self.n_layers, bidirectional = True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #前馈过程\n",
    "        #input尺寸： batch_size, length_seq\n",
    "        embedded = self.embedding(input)\n",
    "        #embedded尺寸：batch_size, length_seq, hidden_size\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # output尺寸：batch_size, length_seq, hidden_size\n",
    "        # hidden尺寸：num_layers * directions, batch_size, hidden_size\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        # 对隐含单元变量全部进行初始化\n",
    "        #num_layers * num_directions, batch, hidden_size\n",
    "        result = Variable(torch.zeros(self.n_layers * 2, batch_size, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "# 定义基于注意力的解码器RNN\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # 词嵌入层\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        \n",
    "        # 注意力网络（一个前馈神经网络）\n",
    "        self.attn = nn.Linear(self.hidden_size * (2 * n_layers + 1), self.max_length)\n",
    "    \n",
    "        # 注意力机制作用完后的结果映射到后面的层\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
    "        \n",
    "        # dropout操作层\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        # 定义一个双向GRU，并设置batch_first为True以方便操作\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, bidirectional = True,\n",
    "                         num_layers = self.n_layers, batch_first = True)\n",
    "        self.out = nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # 解码器的一步操作\n",
    "        # input大小：batch_size, length_seq\n",
    "        embedded = self.embedding(input)\n",
    "        # embedded大小：batch_size, length_seq, hidden_size\n",
    "        embedded = embedded[:, 0, :]\n",
    "        # embedded大小：batch_size, hidden_size\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # 将hidden张量数据转化成batch_size排在第0维的形状\n",
    "        # hidden大小：direction*n_layer, batch_size, hidden_size\n",
    "        temp_for_transpose = torch.transpose(hidden, 0, 1).contiguous()\n",
    "        temp_for_transpose = temp_for_transpose.view(temp_for_transpose.size()[0], -1)\n",
    "        hidden_attn = temp_for_transpose\n",
    "        \n",
    "        # 注意力层的输入\n",
    "        # hidden_attn大小：batch_size, direction*n_layers*hidden_size\n",
    "        input_to_attention = torch.cat((embedded, hidden_attn), 1)\n",
    "        # input_to_attention大小：batch_size, hidden_size * (1 + direction * n_layers)\n",
    "        \n",
    "        # 注意力层输出的权重\n",
    "        attn_weights = F.softmax(self.attn(input_to_attention))\n",
    "        # attn_weights大小：batch_size, max_length\n",
    "        \n",
    "        # 当输入数据不标准的时候，对weights截取必要的一段\n",
    "        attn_weights = attn_weights[:, : encoder_outputs.size()[1]]\n",
    "        # attn_weights大小：batch_size, length_seq_of_encoder\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        # attn_weights大小：batch_size, 1, length_seq 中间的1是为了bmm乘法用的\n",
    "        \n",
    "        # 将attention的weights矩阵乘encoder_outputs以计算注意力完的结果\n",
    "        # encoder_outputs大小：batch_size, seq_length, hidden_size*direction\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_outputs) \n",
    "        # attn_applied大小：batch_size, 1, hidden_size*direction\n",
    "        # bmm: 两个矩阵相乘。忽略第一个batch纬度，缩并时间维度\n",
    "        \n",
    "        # 将输入的词向量与注意力机制作用后的结果拼接成一个大的输入向量\n",
    "        output = torch.cat((embedded, attn_applied[:,0,:]), 1)\n",
    "        # output大小：batch_size, hidden_size * (direction + 1)\n",
    "        \n",
    "        # 将大输入向量映射为GRU的隐含层\n",
    "        output = self.attn_combine(output).unsqueeze(1)\n",
    "        # output大小：batch_size, length_seq, hidden_size\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        # output的结果再dropout\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # 开始解码器GRU的运算\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        \n",
    "        # output大小：batch_size, length_seq, hidden_size * directions\n",
    "        # hidden大小：n_layers * directions, batch_size, hidden_size\n",
    "        \n",
    "        #取出GRU运算最后一步的结果喂给最后一层全链接层\n",
    "        output = self.out(output[:, -1, :])\n",
    "        # output大小：batch_size * output_size\n",
    "        \n",
    "        # 取logsoftmax，计算输出结果\n",
    "        output = F.log_softmax(output)\n",
    "        # output大小：batch_size * output_size\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        # 初始化解码器隐单元，尺寸为n_layers * directions, batch_size, hidden_size\n",
    "        result = Variable(torch.zeros(self.n_layers * 2, batch_size, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "    def initInput(self, batch_size):\n",
    "        return Variable(torch.LongTensor([[Lang.SOS]] * batch_size).type(itype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "n_layers = 1\n",
    "core = 'sru'\n",
    "\n",
    "encoder_t = EncoderRNN(Chi.wordNum(), hidden_size, n_layers = n_layers)\n",
    "decoder_t = AttnDecoderRNN(hidden_size, Eng.wordNum(), n_layers = n_layers , dropout_p=0.5)\n",
    "if use_cuda:\n",
    "    encoder_t.cuda()\n",
    "    decoder_t.cuda()\n",
    "\n",
    "log = []\n",
    "log = training(encoder_t, decoder_t, train, valid, log=log, max_length=MAX_LENGTH, lr=0.0001, n_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "有效句子对： 1836\n",
      "总单词数:\n",
      "Chinese 3324\n",
      "English 3011\n",
      "训练记录： 1653\n",
      "校验记录： 91\n",
      "测试记录： 92\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "# 读取平行语料库\n",
    "# 这是人民日报语料库\n",
    "lines = open('data/chinese.txt', encoding = 'utf-8')\n",
    "chinese = lines.read().strip().split('\\n')\n",
    "lines = open('data/english.txt', encoding = 'utf-8')\n",
    "english = lines.read().strip().split('\\n')\n",
    "print(len(chinese))\n",
    "print(len(english))\n",
    "\n",
    "# 定义两个特殊符号，分别对应句子头和句子尾\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "# 定义一个语言类，方便进行自动的建立、词频的统计等\n",
    "# 在这个对象中，最重要的是两个字典：word2index，index2word\n",
    "# 故名思议，第一个字典是将word映射到索引，第二个是将索引映射到word\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        # 在语言中添加一个新句子，句子是用空格隔开的一组单词\n",
    "        # 将单词切分出来，并分别进行处理\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        # 插入一个单词，如果单词已经在字典中，则更新字典中对应单词的频率\n",
    "        # 同时建立反向索引，可以从单词编号找到单词\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "# 将unicode编码转变为ascii编码\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# 把输入的英文字符串转成小写\n",
    "def normalizeEngString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "# 对输入的单词对做过滤，保证每句话的单词数不能超过MAX_LENGTH\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# 输入一个句子，输出一个单词对应的编码序列\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "# 和上面的函数功能类似，不同在于输出的序列等长＝MAX_LENGTH\n",
    "def indexFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    for i in range(MAX_LENGTH - len(indexes)):\n",
    "        indexes.append(EOS_token)\n",
    "    return(indexes)\n",
    "\n",
    "# 从一个词对到下标\n",
    "def indexFromPair(pair):\n",
    "    input_variable = indexFromSentence(input_lang, pair[0])\n",
    "    target_variable = indexFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)\n",
    "\n",
    "# 从一个列表到句子\n",
    "def SentenceFromList(lang, lst):\n",
    "    result = [lang.index2word[i] for i in lst if i != EOS_token]\n",
    "    if lang.name == 'Chinese':\n",
    "        result = ' '.join(result)\n",
    "    else:\n",
    "        result = ' '.join(result)\n",
    "    return(result)\n",
    "\n",
    "\n",
    "# 计算准确度的函数\n",
    "def rightness(predictions, labels):\n",
    "    \"\"\"计算预测错误率的函数，其中predictions是模型给出的一组预测结果，batch_size行num_classes列的矩阵，labels是数据之中的正确答案\"\"\"\n",
    "    pred = torch.max(predictions.data, 1)[1] # 对于任意一行（一个样本）的输出值的第1个维度，求最大，得到每一行的最大元素的下标\n",
    "    rights = pred.eq(labels.data).sum() #将下标与labels中包含的类别进行比较，并累计得到比较正确的数量\n",
    "    return rights, len(labels) #返回正确的数量和这一次一共比较了多少元素\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 重新处理数据形成训练数据、校验数据与测试数据，主要是MAX_Length更大了\n",
    "# 设置句子的最大长度\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "#对英文做标准化处理\n",
    "pairs = [[chi, normalizeEngString(eng)] for chi, eng in zip(chinese, english)]\n",
    "\n",
    "# 对句子对做过滤，处理掉那些超过MAX_LENGTH长度的句子\n",
    "input_lang = Lang('Chinese')\n",
    "output_lang = Lang('English')\n",
    "pairs = [pair for pair in pairs if filterPair(pair)]\n",
    "print('有效句子对：', len(pairs))\n",
    "\n",
    "# 建立两个字典（中文的和英文的）\n",
    "for pair in pairs:\n",
    "    input_lang.addSentence(pair[0])\n",
    "    output_lang.addSentence(pair[1])\n",
    "print(\"总单词数:\")\n",
    "print(input_lang.name, input_lang.n_words)\n",
    "print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "\n",
    "# 形成训练集，首先，打乱所有句子的顺序\n",
    "random_idx = np.random.permutation(range(len(pairs)))\n",
    "pairs = [pairs[i] for i in random_idx]\n",
    "\n",
    "# 将语言转变为单词的编码构成的序列\n",
    "pairs = [indexFromPair(pair) for pair in pairs]\n",
    "    \n",
    "# 形成训练集、校验集和测试集\n",
    "valid_size = len(pairs) // 10\n",
    "if valid_size > 10000:\n",
    "    valid_size = 10000\n",
    "pairs = pairs[ : - valid_size]\n",
    "valid_pairs = pairs[-valid_size : -valid_size // 2]\n",
    "test_pairs = pairs[- valid_size // 2 :]\n",
    "\n",
    "# 利用PyTorch的dataset和dataloader对象，将数据加载到加载器里面，并且自动分批\n",
    "\n",
    "batch_size = 30 #一撮包含30个数据记录，这个数字越大，系统在训练的时候，每一个周期处理的数据就越多，这样处理越快，但总的数据量会减少\n",
    "\n",
    "print('训练记录：', len(pairs))\n",
    "print('校验记录：', len(valid_pairs))\n",
    "print('测试记录：', len(test_pairs))\n",
    "\n",
    "# 形成训练对列表，用于喂给train_dataset\n",
    "pairs_X = [pair[0] for pair in pairs]\n",
    "pairs_Y = [pair[1] for pair in pairs]\n",
    "valid_X = [pair[0] for pair in valid_pairs]\n",
    "valid_Y = [pair[1] for pair in valid_pairs]\n",
    "test_X = [pair[0] for pair in test_pairs]\n",
    "test_Y = [pair[1] for pair in test_pairs]\n",
    "\n",
    "\n",
    "# 形成训练集\n",
    "train_dataset = DataSet.TensorDataset(torch.LongTensor(pairs_X), torch.LongTensor(pairs_Y))\n",
    "# 形成数据加载器\n",
    "train_loader = DataSet.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers=8)\n",
    "\n",
    "\n",
    "# 校验数据\n",
    "valid_dataset = DataSet.TensorDataset(torch.LongTensor(valid_X), torch.LongTensor(valid_Y))\n",
    "valid_loader = DataSet.DataLoader(valid_dataset, batch_size = batch_size, shuffle = True, num_workers=8)\n",
    "\n",
    "# 测试数据\n",
    "test_dataset = DataSet.TensorDataset(torch.LongTensor(test_X), torch.LongTensor(test_Y))\n",
    "test_loader = DataSet.DataLoader(test_dataset, batch_size = batch_size, shuffle = True, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进程：0% 训练损失：58.4112，校验损失：49.4798，词正确率：31.40%\n",
      "进程：1% 训练损失：47.3585，校验损失：48.5506，词正确率：32.67%\n",
      "进程：2% 训练损失：45.3690，校验损失：48.9167，词正确率：32.56%\n",
      "进程：3% 训练损失：44.5954，校验损失：48.0342，词正确率：32.91%\n",
      "进程：4% 训练损失：43.6130，校验损失：47.8174，词正确率：32.79%\n",
      "进程：5% 训练损失：42.4692，校验损失：47.9798，词正确率：33.14%\n",
      "进程：6% 训练损失：41.4694，校验损失：47.6592，词正确率：33.26%\n",
      "进程：7% 训练损失：40.2288，校验损失：47.8592，词正确率：33.60%\n",
      "进程：8% 训练损失：39.2850，校验损失：47.9711，词正确率：34.88%\n",
      "进程：9% 训练损失：38.0226，校验损失：48.6809，词正确率：32.33%\n",
      "进程：10% 训练损失：37.0786，校验损失：48.2187，词正确率：33.84%\n",
      "进程：11% 训练损失：35.8032，校验损失：49.0991，词正确率：30.93%\n",
      "进程：12% 训练损失：34.8677，校验损失：48.8506，词正确率：32.91%\n",
      "进程：13% 训练损失：34.0522，校验损失：49.0518，词正确率：33.02%\n",
      "进程：14% 训练损失：32.7393，校验损失：49.6140，词正确率：31.28%\n",
      "进程：15% 训练损失：31.7716，校验损失：49.9875，词正确率：32.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-620:\n",
      "Process Process-619:\n",
      "Process Process-617:\n",
      "Process Process-622:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-621:\n",
      "Traceback (most recent call last):\n",
      "Process Process-623:\n",
      "Process Process-618:\n",
      "Traceback (most recent call last):\n",
      "Process Process-624:\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/vtnil/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a8457959413a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# 反向传播开始\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# 开始梯度下降\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 计算准确度的函数\n",
    "def rightness(predictions, labels):\n",
    "    \"\"\"计算预测错误率的函数，其中predictions是模型给出的一组预测结果，batch_size行num_classes列的矩阵，labels是数据之中的正确答案\"\"\"\n",
    "    pred = torch.max(predictions.data, 1)[1] # 对于任意一行（一个样本）的输出值的第1个维度，求最大，得到每一行的最大元素的下标\n",
    "    rights = pred.eq(labels.data).sum() #将下标与labels中包含的类别进行比较，并累计得到比较正确的数量\n",
    "    return rights, len(labels) #返回正确的数量和这一次一共比较了多少元素\n",
    "\n",
    "# 定义两个特殊符号，分别对应句子头和句子尾\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "#定义网络架构\n",
    "hidden_size = 512\n",
    "max_length = MAX_LENGTH\n",
    "n_layers = 1\n",
    "core='sru'\n",
    "\n",
    "\n",
    "encoder = EncoderRNN(Chi.wordNum(), hidden_size, n_layers = n_layers)\n",
    "decoder = AttnDecoderRNN(hidden_size, Eng.wordNum(), dropout_p=0.5,\n",
    "                         max_length = max_length, n_layers = n_layers)\n",
    "\n",
    "# encoder = Encoder(input_lang.n_words, hidden_size, n_layers = n_layers, core=core)\n",
    "# decoder = Decoder(hidden_size, output_lang.n_words, n_layers = n_layers , attn_size=MAX_LENGTH, dropout_p=0.2, core=core)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "\n",
    "print_loss_total = 0  # Reset every print_every\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "#criterion = Batch_NLLLoss\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "# 开始训练周期循环\n",
    "plot_losses = []\n",
    "for epoch in range(num_epoch):\n",
    "    # 将解码器置于训练状态，让dropout工作\n",
    "    decoder.train()\n",
    "    print_loss_total = 0\n",
    "    # 对训练数据进行循环\n",
    "    for data in train:\n",
    "        input_variable = Variable(data[0]).cuda() if use_cuda else Variable(data[0])\n",
    "        # input_variable的大小：batch_size, length_seq\n",
    "        target_variable = Variable(data[1]).cuda() if use_cuda else Variable(data[1])\n",
    "        # target_variable的大小：batch_size, length_seq\n",
    "        \n",
    "        #清空梯度\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden(data[0].size()[0])\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        #编码器开始工作\n",
    "        encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "        # encoder_outputs的大小：batch_size, length_seq, hidden_size*direction\n",
    "        # encoder_hidden的大小：direction*n_layer, batch_size, hidden_size\n",
    "\n",
    "        # 解码器开始工作\n",
    "        decoder_input = Variable(torch.LongTensor([[SOS_token]] * target_variable.size()[0]))\n",
    "        # decoder_input大小：batch_size, length_seq\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        # 将编码器的隐含层单元取值作为编码的结果传递给解码器\n",
    "        decoder_hidden = encoder_hidden\n",
    "        # decoder_hidden大小：direction*n_layer, batch_size, hidden_size\n",
    "\n",
    "        # 同时按照两种方式训练解码器：用教师监督的信息作为下一时刻的输入和不用监督的信息，用自己预测结果作为下一时刻的输入\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        if use_teacher_forcing:\n",
    "            # 用监督信息作为下一时刻解码器的输入\n",
    "            # 开始时间不得循环\n",
    "            for di in range(MAX_LENGTH):\n",
    "                # 输入给解码器的信息包括输入的单词decoder_input, 解码器上一时刻的因曾单元状态，\n",
    "                # 编码器各个时间步的输出结果\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                #decoder_ouput大小：batch_size, output_size\n",
    "                #计算损失函数，得到下一时刻的解码器的输入\n",
    "                loss += criterion(decoder_output, target_variable[:, di])\n",
    "                decoder_input = target_variable[:, di].unsqueeze(1)  # Teacher forcing\n",
    "                # decoder_input大小：batch_size, length_seq\n",
    "        else:\n",
    "            # 没有教师监督，用解码器自己的预测作为下一时刻的输入\n",
    "\n",
    "            # 对时间步进行循环\n",
    "            for di in range(MAX_LENGTH):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                #decoder_ouput大小：batch_size, output_size(vocab_size)\n",
    "                # 获取解码器的预测结果，并用它来作为下一时刻的输入\n",
    "                topv, topi = decoder_output.data.topk(1, dim = 1)\n",
    "                #topi 尺寸：batch_size, k\n",
    "                ni = topi[:, 0]\n",
    "\n",
    "                decoder_input = Variable(ni.unsqueeze(1))\n",
    "                # decoder_input大小：batch_size, length_seq\n",
    "                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "                # 计算损失函数\n",
    "                loss += criterion(decoder_output, target_variable[:, di])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 反向传播开始\n",
    "        loss.backward()\n",
    "        loss = loss.cpu() if use_cuda else loss\n",
    "        # 开始梯度下降\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        print_loss_total += loss.data.numpy()[0]\n",
    "\n",
    "    print_loss_avg = print_loss_total / len(train)\n",
    "        \n",
    "    valid_loss = 0\n",
    "    rights = []\n",
    "    # 将解码器的training设置为False，以便关闭dropout\n",
    "    decoder.eval()\n",
    "    \n",
    "    #对所有的校验数据做循环\n",
    "    for data in valid:\n",
    "        input_variable = Variable(data[0]).cuda() if use_cuda else Variable(data[0])\n",
    "        # input_variable的大小：batch_size, length_seq\n",
    "        target_variable = Variable(data[1]).cuda() if use_cuda else Variable(data[1])\n",
    "        # target_variable的大小：batch_size, length_seq\n",
    "\n",
    "        encoder_hidden = encoder.initHidden(data[0].size()[0])\n",
    "\n",
    "        loss = 0\n",
    "        encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "        # encoder_outputs的大小：batch_size, length_seq, hidden_size*direction\n",
    "        # encoder_hidden的大小：direction*n_layer, batch_size, hidden_size\n",
    "\n",
    "        # decoder_input大小：batch_size, length_seq\n",
    "        decoder_input = Variable(torch.LongTensor([[SOS_token]] * target_variable.size()[0]))\n",
    "        # decoder_input大小：batch_size, length_seq\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        # decoder_hidden大小：direction*n_layer, batch_size, hidden_size\n",
    "\n",
    "        # 开始每一步的预测\n",
    "        for di in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            #decoder_ouput大小：batch_size, output_size(vocab_size)\n",
    "            topv, topi = decoder_output.data.topk(1, dim = 1)\n",
    "            #topi 尺寸：batch_size, k\n",
    "            ni = topi[:, 0]\n",
    "\n",
    "            decoder_input = Variable(ni.unsqueeze(1))\n",
    "            # decoder_input大小：batch_size, length_seq\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            right = rightness(decoder_output, target_variable[:, di])\n",
    "            rights.append(right)\n",
    "            loss += criterion(decoder_output, target_variable[:, di])\n",
    "        loss = loss.cpu() if use_cuda else loss\n",
    "        valid_loss += loss.data.numpy()[0]\n",
    "    # 计算平均损失、准确率等指标并打印输出\n",
    "    right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])\n",
    "    print('进程：%d%% 训练损失：%.4f，校验损失：%.4f，词正确率：%.2f%%' % (epoch * 1.0 / num_epoch * 100, \n",
    "                                                    print_loss_avg,\n",
    "                                                    valid_loss / len(valid),\n",
    "                                                    100.0 * right_ratio))\n",
    "    plot_losses.append([print_loss_avg, valid_loss / len(valid), right_ratio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
